{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import h2j, j2hcj\n",
    "from unicode import join_jamos\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"input/input.txt\", 'r', encoding='utf-8')\n",
    "\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅈㅣㄴㅏㄴㄷㅏㄹ 24ㅇㅣㄹ ㄱㅣㅁ ㅇㅟㅇㅝㄴㅇㅣ ㅊㅓㅇㅡㅁ ㅈㅣㄱㅈㅓㅂ ㅇㅢㅇㅏㄴㅇㅡㄹ ㄴㅐㄴㅗㅎㅇㅡㄴ ㅈㅣ ㅎㅏㄴ ㄷㅏㄹ ㅁㅏㄴㅇㅣㄷㅏ.\\n', 'ㅂㅏㅇㅅㅗㅇㅂㅓㅂ ㅈㅔ44ㅈㅗㄴㅡㄴ kbsㅇㅢ ㄱㅗㅇㅈㅓㄱ ㅊㅐㄱㅇㅣㅁㅇㅡㄹ ㄱㅠㅈㅓㅇㅎㅏㄴ ㅈㅗㅎㅏㅇㅇㅡㄹㅗ △ㅂㅏㅇㅅㅗㅇㅇㅢ ㅁㅗㄱㅈㅓㄱㄱㅘ ㄱㅗㅇㅈㅓㄱ ㅊㅐㄱㅇㅣㅁ, ㅂㅏㅇㅅㅗㅇㅇㅢ ㄱㅗㅇㅈㅓㅇㅅㅓㅇㄱㅘ ㄱㅗㅇㅇㅣㄱㅅㅓㅇㅇㅡㄹ ㅅㅣㄹㅎㅕㄴㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ △ㄱㅜㄱㅁㅣㄴㅇㅣ ㅈㅣㅇㅕㄱㄱㅘ ㅈㅜㅂㅕㄴ ㅇㅕㄱㅓㄴㅇㅔ ㄱㅘㄴㄱㅖㅇㅓㅄㅇㅣ ㅇㅑㅇㅈㅣㄹㅇㅢ ㅂㅏㅇㅅㅗㅇㅅㅓㅂㅣㅅㅡㄹㅡㄹ ㅈㅔㄱㅗㅇㅂㅏㄷㅇㅡㄹ ㅅㅜ ㅇㅣㅆㄷㅗㄹㅗㄱ ㄴㅗㄹㅕㄱㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ △ㅅㅣㅊㅓㅇㅈㅏㅇㅢ ㄱㅗㅇㅇㅣㄱㅇㅔ ㄱㅣㅇㅕㅎㅏㄹ ㅅㅜ ㅇㅣㅆㄴㅡㄴ ㅅㅐㄹㅗㅇㅜㄴ ㅂㅏㅇㅅㅗㅇ ㅍㅡㄹㅗㄱㅡㄹㅐㅁ·ㅂㅏㅇㅅㅗㅇㅅㅓㅂㅣㅅㅡ ㅁㅣㅊ ㅂㅏㅇㅅㅗㅇㄱㅣㅅㅜㄹㅇㅡㄹ ㅇㅕㄴㄱㅜㅎㅏㄱㅗ ㄱㅐㅂㅏㄹㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ △ㄱㅜㄱㄴㅐㅇㅚㄹㅡㄹ ㄷㅐㅅㅏㅇㅇㅡㄹㅗ ㅁㅣㄴㅈㅗㄱㅁㅜㄴㅎㅘㄹㅡㄹ ㅊㅏㅇㄷㅏㄹㅎㅏㄱㅗ ㅁㅣㄴㅈㅗㄱㅇㅢ ㄷㅗㅇㅈㅣㄹㅅㅓㅇㅇㅡㄹ ㅎㅘㄱㅂㅗㅎㅏㄹ ㅅㅜ ㅇㅣㅆㄴㅡㄴ ㅂㅏㅇㅅㅗㅇ ㅍㅡㄹㅗㄱㅡㄹㅐㅁㅇㅡㄹ ㄱㅐㅂㅏㄹㅎㅐ ㅂㅏㅇㅅㅗㅇㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ ㄷㅡㅇㅇㅡㄹㅗ ㄱㅜㅅㅓㅇㄷㅙ ㅇㅣㅆㄷㅏ.\\n', 'ㄱㅣㅁ ㅅㅏㅇㅇㅣㅁㅇㅟㅇㅝㄴㅇㅡㄴ kbsㄱㅏ ㅎㅕㄴㅈㅐ ㅂㅏㅇㅅㅗㅇㅂㅓㅂ ㅈㅔ44ㅈㅗㅇㅢ ㄱㅗㅇㅈㅓㄱ ㅊㅐㄱㅇㅣㅁㅇㅡㄹ ㅈㅔㄷㅐㄹㅗ ㅇㅣㅎㅐㅇㅎㅏㅈㅣ ㅁㅗㅅㅎㅏㄱㅗ ㅇㅣㅆㄴㅡㄴ ㄱㅓㅅㅇㅡㄹㅗ ㅎㅐㅅㅓㄱㅎㅐ ㅂㅏㅇㅌㅗㅇㅇㅟㄱㅏ ㅇㅓㄸㅓㄴ ㅈㅗㅊㅣㄹㅡㄹ ㄴㅐㄹㅕㅇㅑ ㅎㅏㄴㄷㅏㄱㅗ ㅅㅐㅇㄱㅏㄱㅎㅏㄱㅗ ㅇㅣㅆㄷㅏ.\\n', 'ㅂㅏㅇㅌㅗㅇㅇㅟㅇㅔㅅㅓ ㅅㅏㅇㅇㅣㅁㅇㅟㅇㅝㄴㅇㅣ ㅈㅣㄱㅈㅓㅂ ㅇㅏㄴㄱㅓㄴㅇㅡㄹ ㅈㅔㅇㅏㄴㅎㅏㄴ ㄱㅓㅅㅇㅡㄴ ㅇㅣㅂㅓㄴㅇㅣ 2ㅂㅓㄴㅉㅐㄷㅏ.\\n']\n"
     ]
    }
   ],
   "source": [
    "len(lines)\n",
    "print(lines[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hangul = re.compile('[^0-9a-zA-Zㄱ-ㅣ가-힣.,?!\\'\\\" ]+')\n",
    "def process(sent):\n",
    "    sent = hangul.sub('',sent)\n",
    "    sent = sent.replace('\\n', '')\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅈㅣㄴㅏㄴㄷㅏㄹ 24ㅇㅣㄹ ㄱㅣㅁ ㅇㅟㅇㅝㄴㅇㅣ ㅊㅓㅇㅡㅁ ㅈㅣㄱㅈㅓㅂ ㅇㅢㅇㅏㄴㅇㅡㄹ ㄴㅐㄴㅗㅎㅇㅡㄴ ㅈㅣ ㅎㅏㄴ ㄷㅏㄹ ㅁㅏㄴㅇㅣㄷㅏ.', 'ㅂㅏㅇㅅㅗㅇㅂㅓㅂ ㅈㅔ44ㅈㅗㄴㅡㄴ kbsㅇㅢ ㄱㅗㅇㅈㅓㄱ ㅊㅐㄱㅇㅣㅁㅇㅡㄹ ㄱㅠㅈㅓㅇㅎㅏㄴ ㅈㅗㅎㅏㅇㅇㅡㄹㅗ ㅂㅏㅇㅅㅗㅇㅇㅢ ㅁㅗㄱㅈㅓㄱㄱㅘ ㄱㅗㅇㅈㅓㄱ ㅊㅐㄱㅇㅣㅁ, ㅂㅏㅇㅅㅗㅇㅇㅢ ㄱㅗㅇㅈㅓㅇㅅㅓㅇㄱㅘ ㄱㅗㅇㅇㅣㄱㅅㅓㅇㅇㅡㄹ ㅅㅣㄹㅎㅕㄴㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ ㄱㅜㄱㅁㅣㄴㅇㅣ ㅈㅣㅇㅕㄱㄱㅘ ㅈㅜㅂㅕㄴ ㅇㅕㄱㅓㄴㅇㅔ ㄱㅘㄴㄱㅖㅇㅓㅄㅇㅣ ㅇㅑㅇㅈㅣㄹㅇㅢ ㅂㅏㅇㅅㅗㅇㅅㅓㅂㅣㅅㅡㄹㅡㄹ ㅈㅔㄱㅗㅇㅂㅏㄷㅇㅡㄹ ㅅㅜ ㅇㅣㅆㄷㅗㄹㅗㄱ ㄴㅗㄹㅕㄱㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ ㅅㅣㅊㅓㅇㅈㅏㅇㅢ ㄱㅗㅇㅇㅣㄱㅇㅔ ㄱㅣㅇㅕㅎㅏㄹ ㅅㅜ ㅇㅣㅆㄴㅡㄴ ㅅㅐㄹㅗㅇㅜㄴ ㅂㅏㅇㅅㅗㅇ ㅍㅡㄹㅗㄱㅡㄹㅐㅁㅂㅏㅇㅅㅗㅇㅅㅓㅂㅣㅅㅡ ㅁㅣㅊ ㅂㅏㅇㅅㅗㅇㄱㅣㅅㅜㄹㅇㅡㄹ ㅇㅕㄴㄱㅜㅎㅏㄱㅗ ㄱㅐㅂㅏㄹㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ ㄱㅜㄱㄴㅐㅇㅚㄹㅡㄹ ㄷㅐㅅㅏㅇㅇㅡㄹㅗ ㅁㅣㄴㅈㅗㄱㅁㅜㄴㅎㅘㄹㅡㄹ ㅊㅏㅇㄷㅏㄹㅎㅏㄱㅗ ㅁㅣㄴㅈㅗㄱㅇㅢ ㄷㅗㅇㅈㅣㄹㅅㅓㅇㅇㅡㄹ ㅎㅘㄱㅂㅗㅎㅏㄹ ㅅㅜ ㅇㅣㅆㄴㅡㄴ ㅂㅏㅇㅅㅗㅇ ㅍㅡㄹㅗㄱㅡㄹㅐㅁㅇㅡㄹ ㄱㅐㅂㅏㄹㅎㅐ ㅂㅏㅇㅅㅗㅇㅎㅐㅇㅑ ㅎㅏㄴㄷㅏ ㄷㅡㅇㅇㅡㄹㅗ ㄱㅜㅅㅓㅇㄷㅙ ㅇㅣㅆㄷㅏ.', 'ㄱㅣㅁ ㅅㅏㅇㅇㅣㅁㅇㅟㅇㅝㄴㅇㅡㄴ kbsㄱㅏ ㅎㅕㄴㅈㅐ ㅂㅏㅇㅅㅗㅇㅂㅓㅂ ㅈㅔ44ㅈㅗㅇㅢ ㄱㅗㅇㅈㅓㄱ ㅊㅐㄱㅇㅣㅁㅇㅡㄹ ㅈㅔㄷㅐㄹㅗ ㅇㅣㅎㅐㅇㅎㅏㅈㅣ ㅁㅗㅅㅎㅏㄱㅗ ㅇㅣㅆㄴㅡㄴ ㄱㅓㅅㅇㅡㄹㅗ ㅎㅐㅅㅓㄱㅎㅐ ㅂㅏㅇㅌㅗㅇㅇㅟㄱㅏ ㅇㅓㄸㅓㄴ ㅈㅗㅊㅣㄹㅡㄹ ㄴㅐㄹㅕㅇㅑ ㅎㅏㄴㄷㅏㄱㅗ ㅅㅐㅇㄱㅏㄱㅎㅏㄱㅗ ㅇㅣㅆㄷㅏ.']\n"
     ]
    }
   ],
   "source": [
    "lines = [process(x) for x in lines]\n",
    "print(lines[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄱㅕㄹㅈㅓㅇㄷㅙㅆㄷㅏ.', 'ㅇㅣㄴㅎㅏㄷㅐ', 'ctpassㄱㅜㄱㅌㅗㅂㅜ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 단어별로 쪼개는 게 과연 맞을까\n",
    "\"\"\"\n",
    "temp = []\n",
    "for line in lines:\n",
    "    temp+= [x for x in line.split()] \n",
    "temp_list = list(set(temp))\n",
    "print(temp_list[:3])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㄱㅕㄹㅈㅓㅇㄷㅙㅆㄷㅏ.\n",
      "ㅇㅣㄴㅎㅏㄷㅐ\n",
      "ctpassㄱㅜㄱㅌㅗㅂㅜ\n",
      "ㅌㅗㅇㅅㅣㄴㅍㅡㄹㅗㅌㅗㅋㅗㄹ\n",
      "ㄱㅜㅇㅣㄴㄴㅏㄴㅇㅡㄹ\n",
      "Number of items: 144866\n"
     ]
    }
   ],
   "source": [
    "#lines = [process(x) for x in lines]\n",
    "temp = []\n",
    "for line in lines:\n",
    "    temp+= [x for x in line.split()]\n",
    "lines = list(set(temp))\n",
    "print(\"\\n\".join(lines[:5]))\n",
    "print(\"Number of items:\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, '0': 53, '1': 54, '2': 55, '3': 56, '4': 57, '5': 58, '6': 59, '7': 60, '8': 61, '9': 62, 'ㄱ': 63, 'ㄴ': 64, 'ㄷ': 65, 'ㄹ': 66, 'ㅁ': 67, 'ㅂ': 68, 'ㅅ': 69, 'ㅇ': 70, 'ㅈ': 71, 'ㅊ': 72, 'ㅋ': 73, 'ㅍ': 74, 'ㅌ': 75, 'ㅎ': 76, 'ㄲ': 77, 'ㄸ': 78, 'ㅃ': 79, 'ㅆ': 80, 'ㅉ': 81, 'ㄳ': 82, 'ㄵ': 83, 'ㄶ': 84, 'ㄺ': 85, 'ㄻ': 86, 'ㄼ': 87, 'ㄽ': 88, 'ㄾ': 89, 'ㄿ': 90, 'ㅀ': 91, 'ㅄ': 92, 'ㅏ': 93, 'ㅑ': 94, 'ㅓ': 95, 'ㅕ': 96, 'ㅗ': 97, 'ㅛ': 98, 'ㅜ': 99, 'ㅠ': 100, 'ㅡ': 101, 'ㅣ': 102, 'ㅐ': 103, 'ㅒ': 104, 'ㅔ': 105, 'ㅖ': 106, 'ㅘ': 107, 'ㅙ': 108, 'ㅚ': 109, 'ㅝ': 110, 'ㅞ': 111, 'ㅟ': 112, 'ㅢ': 113, ',': 114, '.': 115, '?': 116, '!': 117, \"'\": 118, '\"': 119}\n",
      "{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: 'A', 28: 'B', 29: 'C', 30: 'D', 31: 'E', 32: 'F', 33: 'G', 34: 'H', 35: 'I', 36: 'J', 37: 'K', 38: 'L', 39: 'M', 40: 'N', 41: 'O', 42: 'P', 43: 'Q', 44: 'R', 45: 'S', 46: 'T', 47: 'U', 48: 'V', 49: 'W', 50: 'X', 51: 'Y', 52: 'Z', 53: '0', 54: '1', 55: '2', 56: '3', 57: '4', 58: '5', 59: '6', 60: '7', 61: '8', 62: '9', 63: 'ㄱ', 64: 'ㄴ', 65: 'ㄷ', 66: 'ㄹ', 67: 'ㅁ', 68: 'ㅂ', 69: 'ㅅ', 70: 'ㅇ', 71: 'ㅈ', 72: 'ㅊ', 73: 'ㅋ', 74: 'ㅍ', 75: 'ㅌ', 76: 'ㅎ', 77: 'ㄲ', 78: 'ㄸ', 79: 'ㅃ', 80: 'ㅆ', 81: 'ㅉ', 82: 'ㄳ', 83: 'ㄵ', 84: 'ㄶ', 85: 'ㄺ', 86: 'ㄻ', 87: 'ㄼ', 88: 'ㄽ', 89: 'ㄾ', 90: 'ㄿ', 91: 'ㅀ', 92: 'ㅄ', 93: 'ㅏ', 94: 'ㅑ', 95: 'ㅓ', 96: 'ㅕ', 97: 'ㅗ', 98: 'ㅛ', 99: 'ㅜ', 100: 'ㅠ', 101: 'ㅡ', 102: 'ㅣ', 103: 'ㅐ', 104: 'ㅒ', 105: 'ㅔ', 106: 'ㅖ', 107: 'ㅘ', 108: 'ㅙ', 109: 'ㅚ', 110: 'ㅝ', 111: 'ㅞ', 112: 'ㅟ', 113: 'ㅢ', 114: ',', 115: '.', 116: '?', 117: '!', 118: \"'\", 119: '\"'}\n"
     ]
    }
   ],
   "source": [
    "char_set = list(\" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅍㅌㅎㄲㄸㅃㅆㅉㄳㄵㄶㄺㄻㄼㄽㄾㄿㅀㅄㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣㅐㅒㅔㅖㅘㅙㅚㅝㅞㅟㅢ,.?!\\'\\\"\")\n",
    "char2int = { char_set[x]:x for x in range(len(char_set))}\n",
    "int2char = { char2int[x]:x for x in char_set }\n",
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, '0': 53, '1': 54, '2': 55, '3': 56, '4': 57, '5': 58, '6': 59, '7': 60, '8': 61, '9': 62, 'ㄱ': 63, 'ㄴ': 64, 'ㄷ': 65, 'ㄹ': 66, 'ㅁ': 67, 'ㅂ': 68, 'ㅅ': 69, 'ㅇ': 70, 'ㅈ': 71, 'ㅊ': 72, 'ㅋ': 73, 'ㅍ': 74, 'ㅌ': 75, 'ㅎ': 76, 'ㄲ': 77, 'ㄸ': 78, 'ㅃ': 79, 'ㅆ': 80, 'ㅉ': 81, 'ㄳ': 82, 'ㄵ': 83, 'ㄶ': 84, 'ㄺ': 85, 'ㄻ': 86, 'ㄼ': 87, 'ㄽ': 88, 'ㄾ': 89, 'ㄿ': 90, 'ㅀ': 91, 'ㅄ': 92, 'ㅏ': 93, 'ㅑ': 94, 'ㅓ': 95, 'ㅕ': 96, 'ㅗ': 97, 'ㅛ': 98, 'ㅜ': 99, 'ㅠ': 100, 'ㅡ': 101, 'ㅣ': 102, 'ㅐ': 103, 'ㅒ': 104, 'ㅔ': 105, 'ㅖ': 106, 'ㅘ': 107, 'ㅙ': 108, 'ㅚ': 109, 'ㅝ': 110, 'ㅞ': 111, 'ㅟ': 112, 'ㅢ': 113, ',': 114, '.': 115, '?': 116, '!': 117, \"'\": 118, '\"': 119, '\\t': 120, '\\n': 121, '#': 122}\n",
      "{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: 'A', 28: 'B', 29: 'C', 30: 'D', 31: 'E', 32: 'F', 33: 'G', 34: 'H', 35: 'I', 36: 'J', 37: 'K', 38: 'L', 39: 'M', 40: 'N', 41: 'O', 42: 'P', 43: 'Q', 44: 'R', 45: 'S', 46: 'T', 47: 'U', 48: 'V', 49: 'W', 50: 'X', 51: 'Y', 52: 'Z', 53: '0', 54: '1', 55: '2', 56: '3', 57: '4', 58: '5', 59: '6', 60: '7', 61: '8', 62: '9', 63: 'ㄱ', 64: 'ㄴ', 65: 'ㄷ', 66: 'ㄹ', 67: 'ㅁ', 68: 'ㅂ', 69: 'ㅅ', 70: 'ㅇ', 71: 'ㅈ', 72: 'ㅊ', 73: 'ㅋ', 74: 'ㅍ', 75: 'ㅌ', 76: 'ㅎ', 77: 'ㄲ', 78: 'ㄸ', 79: 'ㅃ', 80: 'ㅆ', 81: 'ㅉ', 82: 'ㄳ', 83: 'ㄵ', 84: 'ㄶ', 85: 'ㄺ', 86: 'ㄻ', 87: 'ㄼ', 88: 'ㄽ', 89: 'ㄾ', 90: 'ㄿ', 91: 'ㅀ', 92: 'ㅄ', 93: 'ㅏ', 94: 'ㅑ', 95: 'ㅓ', 96: 'ㅕ', 97: 'ㅗ', 98: 'ㅛ', 99: 'ㅜ', 100: 'ㅠ', 101: 'ㅡ', 102: 'ㅣ', 103: 'ㅐ', 104: 'ㅒ', 105: 'ㅔ', 106: 'ㅖ', 107: 'ㅘ', 108: 'ㅙ', 109: 'ㅚ', 110: 'ㅝ', 111: 'ㅞ', 112: 'ㅟ', 113: 'ㅢ', 114: ',', 115: '.', 116: '?', 117: '!', 118: \"'\", 119: '\"', 120: '\\t', 121: '\\n', 122: '#'}\n"
     ]
    }
   ],
   "source": [
    "count = len(char_set)\n",
    "codes = [\"\\t\",\"\\n\",'#']\n",
    "for i in range(len(codes)):\n",
    "    code = codes[i]\n",
    "    char2int[code]=count\n",
    "    int2char[count]=code\n",
    "    count+=1\n",
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: uiㅅㅏㅇㅛㅇㅈㅏ\n",
      "Gibberish: uiㅅㅏㅇqㅛㅇㅈㅏ\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#thresh - 0 to 1  얼만큼 바뀌게 할건지(오타가 얼마나 심한지)\n",
    "def gen_gibberish(line,thresh=0.2):\n",
    "    times = int(random.randrange(1,len(line)) * thresh)\n",
    "    '''\n",
    "    Types of replacement: 어떻게 오타가 발생하는지\n",
    "        1.Delete random character.\n",
    "        2.Add random character.\n",
    "        3.Replace a character.\n",
    "        4.Combination?\n",
    "    '''\n",
    "    while times!=0:\n",
    "        times-=1\n",
    "        val = random.randrange(0,10)\n",
    "        if val <= 5:\n",
    "            val = random.randrange(0,10)\n",
    "            index = random.randrange(2,len(line))\n",
    "            if val <=3 :\n",
    "                line = line[:index]+line[index+1:]\n",
    "            else:\n",
    "                insert_index = random.randrange(0, len(char_set))\n",
    "                line = line[:index] + char_set[insert_index] + line[index:]\n",
    "            \n",
    "        else:\n",
    "            index = random.randrange(0, len(char_set))\n",
    "            replace_index = random.randrange(2,len(line))\n",
    "            line = line[:replace_index] + char_set[index] + line[replace_index+1:]\n",
    "    return line\n",
    "\n",
    "sample = lines[5]\n",
    "gib = gen_gibberish(sample)\n",
    "print(\"Original:\", sample)\n",
    "print(\"Gibberish:\", gib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN OF SAMPLES: 20573\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "REPEAT_FACTOR = 1\n",
    "SKIP = int(len(lines)*0.65) #일단 테스트니까 스킵하고 학습하자\n",
    "\n",
    "for line in lines[SKIP:]:\n",
    "    if len(line) > 10:    # 일단 테스트니까 글자 수도 제한 두자\n",
    "        output_text = '\\t' + line + '\\n'\n",
    "        for _ in range(REPEAT_FACTOR):\n",
    "            input_text = gen_gibberish(line)\n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(output_text)\n",
    "print(\"LEN OF SAMPLES:\",len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Enc Len: 61\n",
      "Max Dec Len: 63\n"
     ]
    }
   ],
   "source": [
    "max_enc_len = max([len(x) for x in input_texts])\n",
    "max_dec_len = max([len(x) for x in target_texts])\n",
    "print(\"Max Enc Len:\", max_enc_len)\n",
    "print(\"Max Dec Len:\", max_dec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED ZERO VECTORS\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(input_texts)\n",
    "encoder_input_data = np.zeros((num_samples, max_enc_len, len(char_set)), dtype='float32')\n",
    "decoder_input_data = np.zeros((num_samples, max_dec_len, len(char_set)+2), dtype='float32')\n",
    "decoder_target_data = np.zeros((num_samples, max_dec_len, len(char_set)+2), dtype='float32')\n",
    "print(\"CREATED ZERO VECTORS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED...\n"
     ]
    }
   ],
   "source": [
    "#filling in the enc,dec datas 원핫 벡터인거 같은데\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i,t, char2int[char]] = 1\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, char2int[char]] = 1\n",
    "        if t > 0 :\n",
    "            decoder_target_data[i, t-1, char2int[char]] = 1\n",
    "print(\"COMPLETED...\")\n",
    "    # decoder_target은 한칸씩 당겨진거?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:16:57.661326: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:17:07.358288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 15:17:07.376545: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-03 15:17:07.376560: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-03 15:17:07.377408: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1000\n",
    "latent_dim = 256\n",
    "\n",
    "num_enc_tokens = len(char_set)\n",
    "num_dec_tokens = len(char_set) + 2 # includes \\n \\t\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_enc_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 120)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 122)]  0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        386048      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  388096      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 122)    31354       ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 805,498\n",
      "Trainable params: 805,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None,num_dec_tokens))\n",
    "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "decoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_dec_tokens, activation='softmax')\n",
    "decoder_ouputs = decoder_dense(decoder_ouputs)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n",
    "         ,epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_split = 0.2\n",
    "         )\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "decoder_outputs,state_h,state_c = decoder_lstm(\n",
    "        decoder_inputs,initial_state = decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "encoder_model.save('encoder.h5')\n",
    "decoder_model.save('decoder.h5')\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_dec_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, char2int['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = int2char[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_dec_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_dec_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Wrong sentence:', input_texts[seq_index])\n",
    "    print('Corrected sentence:', decoded_sentence)\n",
    "    print('Ground Truth:',target_texts[seq_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1665044f3270e16212ef6f4dc76378449b1311c55fc70a7845da926a1cb88260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
